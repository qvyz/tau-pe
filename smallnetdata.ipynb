{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.4\n",
      "['L1puppiJetHisto', 'L1gmtMuon', 'L1TrackMET', 'GenPart', 'GenVisTau', 'L1nnPuppiTau', 'L1tkElectron', 'L1EGendcap', 'L1gmtTkMuon', 'L1caloJet', 'L1hpsTau', 'L1puppiJetSC4', 'GenJet', 'L1caloTau', 'L1tkPhoton', 'L1EGbarrel', 'L1nnCaloTau', 'L1TrackHT', 'GenMET', 'L1TrackJet', 'GenJetAK8']\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "print(ak.__version__)\n",
    "from particletools.tables import PYTHIAParticleData as dataname\n",
    "\n",
    "run_local = True\n",
    "debug = True\n",
    "if run_local:\n",
    "    filepath = \"/home/elias/taubus/V37nano/V37nano_VBFHToTauTau*\"\n",
    "else:\n",
    "    filepath = \"/eos/user/e/eleutgeb/menutools/Phase2-L1MenuTools/cache/V37nano/V37nano_VBFHToTauTau*\"\n",
    "\n",
    "debug = True\n",
    "\n",
    "files = glob.glob(filepath)\n",
    "\n",
    "#Get the files:\n",
    "arrays = [ak.from_parquet(file) for file in files]\n",
    "# Get the objectnames:\n",
    "import re\n",
    "if run_local:\n",
    "    regex = re.compile(r'^{}|{}$'.format(re.escape(\"/home/elias/taubus/V37nano/V37nano_VBFHToTauTau_\"), re.escape(\".parquet\")))\n",
    "else: \n",
    "    regex = re.compile(r'^{}|{}$'.format(re.escape(\"/eos/user/e/eleutgeb/menutools/Phase2-L1MenuTools/cache/V37nano/V37nano_VBFHToTauTau_\"), re.escape(\".parquet\")))\n",
    "\n",
    "\n",
    "objectnames = [regex.sub(\"\",file) for file in files]\n",
    "print(objectnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store objects in a dict\n",
    "objectsdict = dict()\n",
    "for idx,item in enumerate(objectnames): \n",
    "    objectsdict[item] = arrays[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calotaus = objectsdict[\"L1caloTau\"]\n",
    "nncalotaus = objectsdict[\"L1nnCaloTau\"]\n",
    "nntaus = objectsdict[\"L1nnPuppiTau\"]\n",
    "jets = objectsdict[\"GenJet\"]\n",
    "taus = objectsdict[\"GenVisTau\"]\n",
    "l1jets = objectsdict[\"L1puppiJetHisto\"]\n",
    "puppijets = objectsdict[\"L1puppiJetHisto\"]\n",
    "elecsbarrel = objectsdict[\"L1EGbarrel\"]\n",
    "elecsendcap = objectsdict[\"L1EGendcap\"]\n",
    "elecsendcap_gt =  ak.pad_none(elecsendcap,6,axis=1,clip=True)\n",
    "elecsbarrel_gt =  ak.pad_none(elecsbarrel,6,axis=1,clip=True)\n",
    "muons = objectsdict[\"L1gmtMuon\"]\n",
    "tkmuons = objectsdict[\"L1gmtTkMuon\"]\n",
    "met =objectsdict[\"L1TrackMET\"]\n",
    "calojets = objectsdict[\"L1caloJet\"]\n",
    "tkele = objectsdict[\"L1tkElectron\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1caloTau_eta', 'L1caloTau_phi', 'L1caloTau_pt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calojets.fields\n",
    "calotaus.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions:\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "x = objectsdict[\"L1EGbarrel\"].fields\n",
    "\n",
    "\n",
    "def prepare_objects(dictobj):\n",
    "    fourvectordict = dict()\n",
    "    standardizeddict = dict()\n",
    "    vector.register_awkward()\n",
    "\n",
    "    for key,value in dictobj.items():\n",
    "        standardizeddict[key] = value\n",
    "        fields = value.fields\n",
    "        for field in fields:\n",
    "            standardname = field.replace(key+\"_\",\"\")\n",
    "            standardizeddict[key][standardname] = dictobj[key][field]\n",
    "            standardizeddict[key] = ak.without_field(standardizeddict[key],field)\n",
    "            dictobj[key] = ak.without_field(value,field)\n",
    "        print(key,\":\",standardizeddict[key].fields)\n",
    "        fourvectordict[key] =  ak.Array(standardizeddict[key],with_name=\"Momentum3D\")\n",
    "    return fourvectordict,standardizeddict\n",
    "\n",
    "\n",
    "\n",
    "    # # combining to numpy\n",
    "def prepare_for_ml(objects,padlist,namelist,pad_val = 0):\n",
    "    pad_arrs = []\n",
    "    var_names = []\n",
    "    for idx,object in enumerate(objects):\n",
    "        topad = padlist[idx]\n",
    "        name = namelist[idx]\n",
    "        \n",
    "        if topad == 1:\n",
    "            pad_arr = object\n",
    "        else:\n",
    "            pad_arr = ak.pad_none(object,topad,axis=1,clip=True)\n",
    "        for i in range(topad):\n",
    "            for var in object.fields:\n",
    "                if topad == 1:\n",
    "                    pad_arrs += [ak.to_numpy(pad_arr[var][:])]\n",
    "                else:\n",
    "                    pad_arrs += [ak.to_numpy( ak.fill_none(pad_arr[var][:,i], pad_val) )]\n",
    "                var_names.append( \"{}_{}_{}\".format(name, i, var) )\n",
    "    return var_names, pad_arrs \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1puppiJetHisto : ['et', 'eta', 'phi', 'pt']\n",
      "L1gmtMuon : ['charge', 'chargeNoPh', 'hwBeta', 'hwEta', 'hwIso', 'hwPhi', 'hwPt', 'hwQual', 'd0', 'eta', 'phi', 'pt', 'z0']\n",
      "L1TrackMET : ['pt']\n",
      "GenPart : ['pt', 'eta', 'phi', 'pdgId', 'statusFlags']\n",
      "GenVisTau : ['status', 'charge', 'genPartIdxMother', 'eta', 'mass', 'phi', 'pt']\n",
      "L1nnPuppiTau : ['charge', 'id', 'passLooseNN', 'passLooseNNMass', 'passLoosePF', 'passMass', 'passTightNN', 'passTightNNMass', 'passTightPF', 'chargedIso', 'dXY', 'eta', 'fullIso', 'phi', 'pt', 'z0']\n",
      "L1tkElectron : ['eleId', 'phoId', 'saId', 'hwEta', 'hwIso', 'hwPhi', 'hwPt', 'hwQual', 'charge', 'eta', 'phi', 'pt', 'relIso', 'z0']\n",
      "L1EGendcap : ['eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1gmtTkMuon : ['charge', 'chargeNoPh', 'hwBeta', 'hwEta', 'hwIso', 'hwPhi', 'hwPt', 'hwQual', 'd0', 'eta', 'phi', 'pt', 'z0']\n",
      "L1caloJet : ['et', 'eta', 'phi', 'pt']\n",
      "L1hpsTau : ['eta', 'phi', 'pt']\n",
      "L1puppiJetSC4 : ['pt', 'eta', 'phi']\n",
      "GenJet : ['pt', 'eta', 'phi', 'partonFlavour']\n",
      "L1caloTau : ['eta', 'phi', 'pt']\n",
      "L1tkPhoton : ['eleId', 'phoId', 'saId', 'hwEta', 'hwIso', 'hwPhi', 'hwPt', 'hwQual', 'eta', 'phi', 'pt', 'relIso']\n",
      "L1EGbarrel : ['eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1nnCaloTau : ['hwIso', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1TrackHT : ['ht', 'mht']\n",
      "GenMET : ['phi', 'pt']\n",
      "L1TrackJet : ['pt', 'eta', 'phi']\n",
      "GenJetAK8 : ['pt', 'eta', 'phi']\n",
      "['eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt']\n"
     ]
    }
   ],
   "source": [
    "padlist = [1,1,5,5,5,5,5]\n",
    "fourvectordict,standardizeddict = prepare_objects(objectsdict)\n",
    "objects = [standardizeddict[\"L1TrackMET\"],standardizeddict[\"L1TrackHT\"]]\n",
    "names = [\"L1TrackMET\",\"L1TrackHT\"]\n",
    "# del standardizeddict[\"L1EGbarrel\"][\"L1EGbarrel_eta\"]\n",
    "print(standardizeddict[\"L1EGbarrel\"].fields)\n",
    "standardizeddict[\"L1puppiJetHisto\"] = ak.without_field(standardizeddict[\"L1puppiJetHisto\"],'et')\n",
    "standardizeddict[\"L1caloJet\"] = ak.without_field(standardizeddict[\"L1caloJet\"],'et')\n",
    "\n",
    "inpnames,y = prepare_for_ml(objects,padlist,names)\n",
    "\n",
    "usecalo = 1\n",
    "if usecalo:\n",
    "    v4l1jets = fourvectordict[\"L1caloJet\"]\n",
    "    v4nntaus = fourvectordict[\"L1caloTau\"]\n",
    "    l1jets = standardizeddict[\"L1caloJet\"]\n",
    "    nntaus = standardizeddict[\"L1caloTau\"]\n",
    "else: \n",
    "    v4l1jets = fourvectordict[\"L1puppiJetHisto\"]\n",
    "    l1jets = standardizeddict[\"L1puppiJetHisto\"]\n",
    "    v4nntaus = fourvectordict[\"L1nnPuppiTau\"]\n",
    "    nntaus = standardizeddict[\"L1nnPuppiTau\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "L1TrackMET_0_pt 35.84375\n",
      "L1TrackHT_0_ht 83.5\n",
      "L1TrackHT_0_mht 22.53125\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# x = nntaus[0][0]\n",
    "# x.values()\n",
    "objarray = np.vstack(y)\n",
    "print(len(objarray[:,0]))\n",
    "len(inpnames)\n",
    "for idx,name in enumerate(inpnames):\n",
    "    print(name,objarray[idx,0])\n",
    "    \n",
    "#delta arrays:\n",
    "\n",
    "import vector\n",
    "if debug:\n",
    "    print(len(objarray[:,0]))\n",
    "\n",
    "len(nntaus)\n",
    "\n",
    "\n",
    "def objdeltas(obj1,obj2):\n",
    "    obj1  = obj2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2666872.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.sum(standardizeddict[\"L1tkElectron\"].pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99701 * var * Momentum3D[\n",
      "    charge: int32,\n",
      "    chargeNoPh: int32,\n",
      "    hwBeta: int32,\n",
      "    hwEta: int32,\n",
      "    hwIso: int32,\n",
      "    hwPhi: int32,\n",
      "    hwPt: int32,\n",
      "    hwQual: int32,\n",
      "    d0: float32,\n",
      "    eta: float32,\n",
      "    phi: float32,\n",
      "    pt: float32,\n",
      "    z0: float32\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fourvectordict[\"L1gmtMuon\"].type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DeltaRcalc(o1,o2):\n",
    "    #returns the delta in the dimensions of the second object with delta R  first objects, second\n",
    "    combo = ak.cartesian({\"offl\": o1,\"trgobj\": o2}, nested=True ,axis=-1)\n",
    "    off,trg = ak.unzip(combo)\n",
    "    deltaR = off.deltaR(trg)\n",
    "    return deltaR\n",
    "\n",
    "def mapObjects(o1,o2):\n",
    "    combo = ak.cartesian({\"offl\": o1,\"trgobj\": o2}, nested=True)\n",
    "    off,trg = ak.unzip(combo)\n",
    "    return off,trg\n",
    "\n",
    "\n",
    "def sortbydeltaR(o1,o2):\n",
    "    o1cart,o2cart = mapObjects(o1,o2)\n",
    "    dR = DeltaRcalc(o1,o2)\n",
    "    sorteddR = ak.argsort(dR)    \n",
    "    return sorteddR\n",
    "\n",
    "def dRtonumpy(o1,dr):\n",
    "    o1dR = o1[dr]\n",
    "    ttpad = ak.pad_none(o1dR,3,clip=True)\n",
    "    nptt = ak.to_numpy(ttpad)\n",
    "    npreg = stuctured_to_unstructured(nptt)\n",
    "    return ak.fill_none(npreg)\n",
    "\n",
    "\n",
    "    \n",
    "test = fourvectordict[\"L1gmtMuon\"]\n",
    "deltartaumuons = DeltaRcalc(test,v4nntaus)\n",
    "\n",
    "# Gen Taus vs NN Taus\n",
    "combo = ak.cartesian({\"offl\": fourvectordict[\"GenVisTau\"],\"trgobj\": fourvectordict[\"L1caloTau\"]}, nested=True)\n",
    "offmu,trgobj = ak.unzip(combo)\n",
    "deltaRtaus_nnTaus = trgobj.deltaR(offmu) #magic?!\n",
    "\n",
    "# L1Jets vs NN Taus\n",
    "combo = ak.cartesian({\"offl\": v4l1jets,\"trgobj\": v4nntaus}, nested=True)\n",
    "offmu,trgobj = ak.unzip(combo)\n",
    "deltaRl1jets_nnTaus = offmu.deltaR(trgobj) #magic?!\n",
    "\n",
    "combo = ak.cartesian({\"trgobj\": v4nntaus,\"offl\": v4l1jets}, nested=True)\n",
    "offmu,trgobj = ak.unzip(combo)\n",
    "deltaRnnTaus_l1jets = trgobj.deltaR(offmu) #magic?!\n",
    "\n",
    "\n",
    "\n",
    "#map \n",
    "combo_pt =  ak.cartesian({\"a\": v4l1jets, \"b\":v4nntaus},nested=True)\n",
    "trgjets,trgtaus = ak.unzip(combo_pt)\n",
    "\n",
    "ptpercentage = trgjets.pt/trgtaus.pt\n",
    "\n",
    "\n",
    "\n",
    "input_jets = []\n",
    "input_taus = []\n",
    "input_dRs = []\n",
    "input_ptperc = []\n",
    "input_is_true_tau = []\n",
    "input_eles_barrel = []\n",
    "input_eles_endcap = []\n",
    "\n",
    "input_muons = []\n",
    "input_false_eles = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l1jets_near_nnTaus = (deltaRl1jets_nnTaus < 0.3)\n",
    "nntaus_near_taus_old = ak.any((deltaRtaus_nnTaus < 0.3),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{eta: 1.09, phi: 1.35, pt: 117}, {...}, {eta: -4.1, phi: -1.18, pt: 84}]\n"
     ]
    }
   ],
   "source": [
    "#calculate DeltaR:\n",
    "dr = DeltaRcalc(fourvectordict[\"GenVisTau\"],fourvectordict[\"L1caloTau\"])\n",
    "# make flat pattern with empty fields, in this case where there are trigger taus and no gen taus:\n",
    "drleng = ak.num(dr)\n",
    "emptydr = drleng==0\n",
    "#calculate the non matched taus,this is used only for the mask:\n",
    "# Dr_greater = ak.any(dr >= 0.3,axis=-2,keepdims=True, mask_identity=False)  \n",
    "Dr_smaller = ak.any(dr < 0.3,axis=-2,keepdims=True, mask_identity=False)  \n",
    "Dr_greater = ~Dr_smaller\n",
    "\n",
    "temptaus = ak.mask(fourvectordict[\"L1caloTau\"],Dr_greater)\n",
    "# print(\"near\",v4truetaus[15])\n",
    "temp_length = ak.num(temptaus,axis=2)\n",
    "temp_mask = temp_length==0\n",
    "Dr_greater_flat = ak.flatten(Dr_greater,axis=-1)\n",
    "falsepattern = ak.where(emptydr,temp_mask,Dr_greater_flat)\n",
    "nntaus_no_near_taus = falsepattern\n",
    "nntaus_near_taus = ~ nntaus_no_near_taus\n",
    "tt = ak.mask(standardizeddict[\"L1caloTau\"],falsepattern)\n",
    "\n",
    "\n",
    "# nntaus_near_taus = ak.any(dr < 3,axis=1,keepdims=True, mask_identity=False)  \n",
    "# nntaus_no_near_taus = ak.any(dr >= 3,axis=1,keepdims=True, mask_identity=False)  \n",
    "\n",
    "#we also have to check where there are no hadronic taus in the event\n",
    "#done by counting dr values and then masking the list\n",
    "# drleng = ak.num(dr)\n",
    "# emptydr = ak.any(drleng == 0,axis=0, keepdims=True, mask_identity=False)\n",
    "# nntaus_near_taus = ak.any((dr < 0.3),axis=1)\n",
    "v4truetaus = ak.drop_none(ak.mask(fourvectordict[\"L1caloTau\"],nntaus_near_taus),axis=1)\n",
    "v4falsetaus =  ak.drop_none(ak.mask(fourvectordict[\"L1caloTau\"],nntaus_no_near_taus),axis=1)\n",
    "combo = ak.cartesian({\"trgobj\": v4truetaus,\"offl\": v4l1jets}, nested=True)\n",
    "offmu,trgobj = ak.unzip(combo)\n",
    "deltaRl1jets_nnTaus = offmu.deltaR(trgobj) #magic?!\n",
    "truejets = l1jets[ak.argmin(deltaRl1jets_nnTaus,axis=-1)]\n",
    "combo = ak.cartesian({\"trgobj\": v4falsetaus,\"offl\": v4l1jets}, nested=True)\n",
    "offmu,trgobj = ak.unzip(combo)\n",
    "deltaRl1jets_nnTaus = offmu.deltaR(trgobj) #magic?!\n",
    "falsejets = l1jets[ak.argmin(deltaRl1jets_nnTaus,axis=-1)]\n",
    "truetaus = ak.drop_none(ak.mask(standardizeddict[\"L1caloTau\"],nntaus_near_taus),axis=1)\n",
    "falsetaus = ak.drop_none(ak.mask(standardizeddict[\"L1caloTau\"],nntaus_no_near_taus),axis=1)\n",
    "print(v4falsetaus[15])\n",
    "# test =  + ak.mask(standardizeddict[\"L1caloTau\"],emptydr)  \n",
    "# falsetaus = ak.concatenate(falsetaus,ak.drop_none(test,axis=-1),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]\n",
      "[[False, False, False]]\n"
     ]
    }
   ],
   "source": [
    "print(nntaus_near_taus_old[0])\n",
    "Dr_smaller = ak.any(dr <= 0.3,axis=-2,keepdims=True, mask_identity=False)  \n",
    "#[False, True, True, False, False, False]\n",
    "print(Dr_smaller[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]\n",
      "[0.0382, 1.21]\n",
      "{False: 251829, True: 88096}\n",
      "339925\n",
      "339925 * Momentum3D[\n",
      "    eta: float32,\n",
      "    phi: float32,\n",
      "    pt: float32\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(nntaus_near_taus[0])\n",
    "# for i in range(1000):\n",
    "#     if not ak.almost_equal(nntaus_near_taus[i], nntaus_near_taus_old[i]):\n",
    "#         print(nntaus_near_taus[i])\n",
    "#         print(nntaus_near_taus_old[i])\n",
    "#         print(i)\n",
    "# # v4truetaus[15]\n",
    "# print(nntaus_near_taus_old[0])\n",
    "# print(nntaus_near_taus[0])\n",
    "# Dr_smaller_test = ak.any(dr < 3,axis=None,keepdims=True, mask_identity=False)  \n",
    "\n",
    "# for item in standardizeddict[\"L1caloTau\"][0]:\n",
    "#     print(item)\n",
    "\n",
    "# for item in standardizeddict[\"GenVisTau\"][0]:\n",
    "#     print(item['eta'])\n",
    "#     print(item['phi'])\n",
    "        \n",
    "# print(\"dr\",dr[0])\n",
    "# print(\"drnew\",Dr_smaller_test[0])\n",
    "print(fourvectordict[\"L1caloTau\"][1][1].deltaR(fourvectordict[\"GenVisTau\"][1]))\n",
    "x = ak.flatten(nntaus_near_taus,axis=None)\n",
    "unique, counts = np.unique(x, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(len(x))\n",
    "\n",
    "# print(len())\n",
    "# # {0.0: 216168, 1.0: 88096}\n",
    "\n",
    "# print(len(x))\n",
    "# for i in range(50):\n",
    "#     print(len(standardizeddict[\"L1caloTau\"][i]))\n",
    "#     print(len(Dr_greater_flat[i]))\n",
    "\n",
    "ak.flatten(fourvectordict[\"L1caloTau\"],axis=-1).type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]\n",
      "[{eta: -3.93, phi: 1.44, pt: 50.6, test: False}, {...}, {eta: -0.917, ...}]\n"
     ]
    }
   ],
   "source": [
    "ttestt = ak.num(fourvectordict[\"L1caloTau\"])\n",
    "xxx = ak.unflatten(x,ttestt)\n",
    "print(xxx[0])\n",
    "xx = ak.with_field(fourvectordict[\"L1caloTau\"],xxx,\"test\")\n",
    "print(xx[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ak.any(fourvectordict[\"L1caloTau\"][1][1].deltaR(fourvectordict[\"GenVisTau\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'eta', 'phi', 'pt']\n",
      "after ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual']\n",
      "before ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'eta', 'phi', 'pt', 'z0']\n",
      "after ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'z0']\n",
      "L1puppiJetHisto : ['eta', 'phi', 'pt']\n",
      "L1gmtMuon : ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual']\n",
      "L1TrackMET : ['pt']\n",
      "GenPart : ['pt', 'eta', 'phi', 'pdgId', 'statusFlags']\n",
      "GenVisTau : ['status', 'charge', 'genPartIdxMother', 'eta', 'mass', 'phi', 'pt']\n",
      "L1nnPuppiTau : ['id', 'passLooseNN', 'passLooseNNMass', 'passLoosePF', 'passMass', 'passTightNN', 'passTightNNMass', 'passTightPF', 'chargedIso', 'eta', 'phi', 'pt', 'z0']\n",
      "L1tkElectron : ['eleId', 'phoId', 'saId', 'hwQual', 'charge', 'eta', 'phi', 'pt', 'relIso', 'z0']\n",
      "L1EGendcap : ['eleId', 'saId', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1gmtTkMuon : ['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'z0']\n",
      "L1caloJet : ['eta', 'phi', 'pt']\n",
      "L1hpsTau : ['eta', 'phi', 'pt']\n",
      "L1puppiJetSC4 : ['pt', 'eta', 'phi']\n",
      "GenJet : ['pt', 'eta', 'phi', 'partonFlavour']\n",
      "L1caloTau : ['eta', 'phi', 'pt']\n",
      "L1tkPhoton : ['eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'relIso']\n",
      "L1EGbarrel : ['eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1nnCaloTau : ['hwIso', 'hwQual', 'eta', 'phi', 'pt']\n",
      "L1TrackHT : ['ht', 'mht']\n",
      "GenMET : ['phi', 'pt']\n",
      "L1TrackJet : ['pt', 'eta', 'phi']\n",
      "GenJetAK8 : ['pt', 'eta', 'phi']\n",
      "251829\n",
      "251829\n",
      "251829\n",
      "251829\n",
      "251829\n",
      "251829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# muonstrue,b = ak.broadcast_arrays(standardizeddict[\"L1gmtMuon\"],v4truetaus, depth_limit=1)\n",
    "# v4muonstrue,b = ak.broadcast_arrays(fourvectordict[\"L1gmtMuon\"],v4truetaus, depth_limit=2)\n",
    "# test = ak.drop_none(v4muonstrue)\n",
    "# drmu = DeltaRcalc(v4truetaus,fourvectordict[\"L1gmtMuon\"])\n",
    "# test = ak.argsort(drmu)\n",
    "# test.type.show()\n",
    "# drmu.type.show()\n",
    "\n",
    "# b.type.show()\n",
    "# x = fourvectordict[\"L1gmtMuon\"]\n",
    "# v4muonstrue[test]\n",
    "# for l in range(len(drmu)):\n",
    "#     tt = fourvectordict[\"L1gmtMuon\"][l]\n",
    "#     print(l)\n",
    "#     tt[test[l]]\n",
    "# fourvectordict[\"L1gmtMuon\"][547]\n",
    "# standardizeddict[\"L1gmtMuon\"][ak.argsort(deltartaumuons,axis=1)]\n",
    "\n",
    "def PrepareReldR(a,b,c,d,sort):\n",
    "    f,g = mapObjects(a,b)\n",
    "    drmu = DeltaRcalc(c,d)\n",
    "    tts = ak.argsort(drmu)\n",
    "    muonssorted = g[tts]\n",
    "    # test = ak.any(nntaus_near_taus,axis=1)\n",
    "    mm = ak.flatten(muonssorted[ak.any(sort,axis=1)],axis=1)\n",
    "    return mm\n",
    "# v4l1jets = fourvectordict[\"L1puppiJetHisto\"]\n",
    "\n",
    "#remove empty fields:\n",
    "for key, value in standardizeddict.items():\n",
    "    for field in value.fields:\n",
    "        if ak.sum(value[field]) == 0:\n",
    "            standardizeddict[key] = ak.without_field(standardizeddict[key],field)\n",
    "#remove redundant information:\n",
    "removal = [\"L1gmtMuon\",\n",
    "\"L1gmtTkMuon\",\n",
    "]            \n",
    "for i in removal:\n",
    "    print(\"before\",standardizeddict[i].fields)\n",
    "    standardizeddict[i] = ak.without_field(standardizeddict[i],'pt')\n",
    "    standardizeddict[i] = ak.without_field(standardizeddict[i],'eta')\n",
    "    standardizeddict[i] = ak.without_field(standardizeddict[i],'phi')\n",
    "    print(\"after\",standardizeddict[i].fields)\n",
    "\n",
    "\n",
    "for key, value in standardizeddict.items():\n",
    "    print(key,\":\",value.fields)\n",
    "# truecalojets = PrepareReldR(truetaus,standardizeddict[\"L1caloJet\"],v4truetaus,fourvectordict[\"L1caloJet\"],nntaus_near_taus)\n",
    "# falsecalojets = PrepareReldR(falsetaus,standardizeddict[\"L1caloJet\"],v4falsetaus,fourvectordict[\"L1caloJet\"],~nntaus_near_taus)\n",
    "truemuons = PrepareReldR(truetaus,standardizeddict[\"L1gmtMuon\"],v4truetaus,fourvectordict[\"L1gmtMuon\"],nntaus_near_taus)\n",
    "falsemuons = PrepareReldR(falsetaus,standardizeddict[\"L1gmtMuon\"],v4falsetaus,fourvectordict[\"L1gmtMuon\"],~nntaus_near_taus)\n",
    "truetkmuons = PrepareReldR(truetaus,standardizeddict[\"L1gmtTkMuon\"],v4truetaus,fourvectordict[\"L1gmtTkMuon\"],nntaus_near_taus)\n",
    "falsetkmuons = PrepareReldR(falsetaus,standardizeddict[\"L1gmtTkMuon\"],v4falsetaus,fourvectordict[\"L1gmtTkMuon\"],~nntaus_near_taus)\n",
    "trueg = PrepareReldR(truetaus,standardizeddict[\"L1tkPhoton\"],v4truetaus,fourvectordict[\"L1tkPhoton\"],nntaus_near_taus)\n",
    "falseg = PrepareReldR(falsetaus,standardizeddict[\"L1tkPhoton\"],v4falsetaus,fourvectordict[\"L1tkPhoton\"],~nntaus_near_taus)\n",
    "trueeles = PrepareReldR(truetaus,standardizeddict[\"L1tkElectron\"],v4truetaus,fourvectordict[\"L1tkElectron\"],nntaus_near_taus)\n",
    "falseeles = PrepareReldR(falsetaus,standardizeddict[\"L1tkElectron\"],v4falsetaus,fourvectordict[\"L1tkElectron\"],~nntaus_near_taus)\n",
    "truej = PrepareReldR(truetaus,standardizeddict[\"L1puppiJetHisto\"],v4truetaus,fourvectordict[\"L1puppiJetHisto\"],nntaus_near_taus)\n",
    "falsej = PrepareReldR(falsetaus,standardizeddict[\"L1puppiJetHisto\"],v4falsetaus,fourvectordict[\"L1puppiJetHisto\"],~nntaus_near_taus)\n",
    "truecaloeb = PrepareReldR(truetaus,standardizeddict[\"L1EGbarrel\"],v4truetaus,fourvectordict[\"L1EGbarrel\"],nntaus_near_taus)\n",
    "falsecaloeb = PrepareReldR(falsetaus,standardizeddict[\"L1EGbarrel\"],v4falsetaus,fourvectordict[\"L1EGbarrel\"],~nntaus_near_taus)\n",
    "truecaloee = PrepareReldR(truetaus,standardizeddict[\"L1EGendcap\"],v4truetaus,fourvectordict[\"L1EGendcap\"],nntaus_near_taus)\n",
    "falsecaloee = PrepareReldR(falsetaus,standardizeddict[\"L1EGendcap\"],v4falsetaus,fourvectordict[\"L1EGendcap\"],~nntaus_near_taus)\n",
    "truecalojets = PrepareReldR(truetaus,standardizeddict[\"L1caloJet\"],v4truetaus,fourvectordict[\"L1caloJet\"],nntaus_near_taus)\n",
    "falsecalojets = PrepareReldR(falsetaus,standardizeddict[\"L1caloJet\"],v4falsetaus,fourvectordict[\"L1caloJet\"],~nntaus_near_taus)\n",
    "tobemapped = [truemuons,trueeles,trueg,truej, truetkmuons,truecalojets]\n",
    "ls = truemuons.fields + trueeles.fields + trueg.fields + truej.fields + truetkmuons.fields + truecalojets.fields\n",
    "names = [\"L1gmtMuon\",\n",
    "\"L1tkElectron\",\n",
    "\"L1tkPhoton\",\n",
    "\"L1puppiJetHisto\",\n",
    "\"L1gmtTkMuon\",\n",
    "\"L1caloJet\"\n",
    "]\n",
    "#remove non physical vals:\n",
    "\n",
    "padlist = [2,5,5,5,3,4]\n",
    "drtruenames, drtruevals = prepare_for_ml(tobemapped,padlist,names)\n",
    "\n",
    "\n",
    "tobemapped = [falsemuons,falseeles,falseg,falsej, falsetkmuons,falsecalojets]\n",
    "for item in tobemapped:\n",
    "    print(len(item))\n",
    "ls = falsemuons.fields + falseeles.fields  + falseg.fields + falsej.fields + falsecaloeb.fields + falsecaloee.fields + falsetkmuons.fields\n",
    "#padlist = [5,5,5,5]\n",
    "drfnames, temp = prepare_for_ml(tobemapped,padlist,names)\n",
    "\n",
    "drfvals = np.vstack(temp)\n",
    "drtruevals = np.vstack(drtruevals)\n",
    "drtruevals =np.swapaxes(drtruevals,0,1)\n",
    "drfvals =np.swapaxes(drfvals,0,1)\n",
    "drfvals = ak.to_numpy(drfvals)\n",
    "drtruevals = ak.to_numpy(drtruevals)\n",
    "\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "# drfvals = structured_to_unstructured(drfvals)\n",
    "# drtruevals = structured_to_unstructured(drtruevals)\n",
    "\n",
    "# f,g = mapObjects(truetaus,standardizeddict[\"L1gmtMuon\"])\n",
    "# drmu = DeltaRcalc(v4truetaus,fourvectordict[\"L1gmtMuon\"])\n",
    "# # tt = f.deltaR(g)\n",
    "# tts = ak.argsort(drmu)\n",
    "# # bb,muons = ak.unzip(ak.cartesian([truetaus,standardizeddict[\"L1gmtMuon\"]]))\n",
    "# muonssorted = g[tts]\n",
    "# # test = ak.any(nntaus_near_taus,axis=1)\n",
    "# mm = ak.flatten(muonssorted[ak.any(nntaus_near_taus,axis=1)],axis=1)\n",
    "# mm.type.show()\n",
    "# ttflat = ak.flatten(truetaus)\n",
    "# ttflat.type.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'eleId', 'phoId', 'saId', 'hwQual', 'charge', 'eta', 'phi', 'pt', 'relIso', 'z0', 'eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'relIso', 'eta', 'phi', 'pt', 'eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'eleId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'z0']\n"
     ]
    }
   ],
   "source": [
    "print(ls)\n",
    "debug= 0\n",
    "if(debug==1):\n",
    "    for item in tobemapped:\n",
    "        print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tt = fourvectordict[\"L1gmtMuon\"][547]\n",
    "# print(len(v4truetaus[15]))\n",
    "# print(len(v4falsetaus[15]))\n",
    "# print(len(standardizeddict[\"L1caloTau\"][15]))\n",
    "# print(len(fourvectordict[\"GenVisTau\"][15]))\n",
    "# nntaus_near_taus = (dr < 0.3)\n",
    "\n",
    "# tt = ak.mask(standardizeddict[\"L1caloTau\"],emptydr)\n",
    "# print(emptydr)\n",
    "# print(ak.is_none(tt[15]))\n",
    "# print(\"tt\",tt[15])\n",
    "# tt2 = ak.is_none(tt,axis=-2)\n",
    "# print(tt2[15])\n",
    "# falsetaus = ak.mask(standardizeddict[\"L1caloTau\"], dr > 3)\n",
    "# print(falsetaus)\n",
    "\n",
    "# print(\"dr\",nntaus_near_taus[15])\n",
    "# combo = ak.cartesian({\"offl\": fourvectordict[\"GenVisTau\"],\"trgobj\": fourvectordict[\"L1caloTau\"]}, nested=True)\n",
    "# offmu,trgobj = ak.unzip(combo)\n",
    "# deltaRtaus_nnTaus = offmu.deltaR(trgobj) \n",
    "# print(offmu[15])\n",
    "# print(trgobj[15])\n",
    "# deltaRtaus_nnTaus[15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drleng = ak.num(dr)\n",
    "# print(\"len\",drleng)\n",
    "# emptydr = drleng==0\n",
    "# print(\"e,\",emptydr)\n",
    "# nntaus_near_taus = ak.any(dr < 3,axis=-2,keepdims=True, mask_identity=False)  \n",
    "# # v4truetaus = ak.mask(fourvectordict[\"L1caloTau\"],nntaus_near_taus)\n",
    "# print(\"near\",v4truetaus[15])\n",
    "# xx = ak.num(v4truetaus,axis=2)\n",
    "# xy = xx==0\n",
    "# nntaus_near_taus = ak.flatten(nntaus_near_taus,axis=-1)\n",
    "# pattern = ak.where(emptydr,xy,nntaus_near_taus)\n",
    "# print(\"pat\",pattern)\n",
    "# print(xy[15])\n",
    "# nntaus_near_taus = ak.mask(dr, dr <= 3)  \n",
    "# # v4truetaus = fourvectordict[\"L1caloTau\"][nntaus_near_taus] \n",
    "# # testtau = ak.mask(standardizeddict[\"L1caloTau\"][15],dr[15] <= 3)                                                                                  \n",
    "# # print(\"testtau\",testtau[0])   \n",
    "# # patternflat = ak.flatten(pattern,axis=-1)\n",
    "# tt = ak.mask(standardizeddict[\"L1caloTau\"],pattern)\n",
    "# print(standardizeddict[\"L1caloTau\"][0][1])\n",
    "# for i in range(10):\n",
    "#     print(pattern[i])\n",
    "#     print(\"testtau\",(tt[i]))\n",
    "\n",
    "# print(\"chcch\",pattern[15])\n",
    "# print(\"testtau\",(tt[15]))      \n",
    "# # print(\"testtau\",(tt[0][0]))   \n",
    "\n",
    "# testtau = ak.mask(standardizeddict[\"L1caloTau\"],nntaus_near_taus) \n",
    "\n",
    "# # test = ak.concatenate([tt,testtau], axis=1)\n",
    "# print(\"conc\",ak.drop_none(test,axis=-1))\n",
    "# print(testtau[0][0])\n",
    "# print(standardizeddict[\"L1caloTau\"][0][1])\n",
    "# # ttest2 = ak.drop_none(test)\n",
    "# print(ttest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 0\n",
    "if debug:\n",
    "    for item,value in enumerate(standardizeddict[\"L1caloTau\"]):\n",
    "        if len(value) != len(v4truetaus[item]) + len(v4falsetaus[item]):\n",
    "            print(item)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare trues:\n",
    "#broadcast other objects to the form of truetaus:\n",
    "#first flip axis\n",
    "objswapped =np.swapaxes(objarray,0,1)\n",
    "objswappedak = ak.Array(objswapped)\n",
    "#output arrays\n",
    "tobj,b = ak.broadcast_arrays(objswappedak[:,np.newaxis],truetaus)\n",
    "tobjflat =ak.flatten(tobj)\n",
    "nptobjflat = ak.to_numpy(tobjflat)\n",
    "#transform truetaus to numpy\n",
    "ttflat = ak.flatten(truetaus)\n",
    "npttflat = ak.to_numpy(ttflat)\n",
    "tjflat = ak.flatten(truejets)\n",
    "nptjflat = ak.to_numpy(tjflat)\n",
    "\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "unstructuredtruetaus = structured_to_unstructured(npttflat)\n",
    "unstructuredtruejets = structured_to_unstructured(nptjflat)\n",
    "\n",
    "trues = np.concatenate((unstructuredtruetaus,drtruevals),axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "['calotaus_eta', 'calotaus_phi', 'calotaus_pt', 'L1gmtMuon_0_charge', 'L1gmtMuon_0_hwBeta', 'L1gmtMuon_0_hwEta', 'L1gmtMuon_0_hwPhi', 'L1gmtMuon_0_hwPt', 'L1gmtMuon_0_hwQual', 'L1gmtMuon_1_charge', 'L1gmtMuon_1_hwBeta', 'L1gmtMuon_1_hwEta', 'L1gmtMuon_1_hwPhi', 'L1gmtMuon_1_hwPt', 'L1gmtMuon_1_hwQual', 'L1tkElectron_0_eleId', 'L1tkElectron_0_phoId', 'L1tkElectron_0_saId', 'L1tkElectron_0_hwQual', 'L1tkElectron_0_charge', 'L1tkElectron_0_eta', 'L1tkElectron_0_phi', 'L1tkElectron_0_pt', 'L1tkElectron_0_relIso', 'L1tkElectron_0_z0', 'L1tkElectron_1_eleId', 'L1tkElectron_1_phoId', 'L1tkElectron_1_saId', 'L1tkElectron_1_hwQual', 'L1tkElectron_1_charge', 'L1tkElectron_1_eta', 'L1tkElectron_1_phi', 'L1tkElectron_1_pt', 'L1tkElectron_1_relIso', 'L1tkElectron_1_z0', 'L1tkElectron_2_eleId', 'L1tkElectron_2_phoId', 'L1tkElectron_2_saId', 'L1tkElectron_2_hwQual', 'L1tkElectron_2_charge', 'L1tkElectron_2_eta', 'L1tkElectron_2_phi', 'L1tkElectron_2_pt', 'L1tkElectron_2_relIso', 'L1tkElectron_2_z0', 'L1tkElectron_3_eleId', 'L1tkElectron_3_phoId', 'L1tkElectron_3_saId', 'L1tkElectron_3_hwQual', 'L1tkElectron_3_charge', 'L1tkElectron_3_eta', 'L1tkElectron_3_phi', 'L1tkElectron_3_pt', 'L1tkElectron_3_relIso', 'L1tkElectron_3_z0', 'L1tkElectron_4_eleId', 'L1tkElectron_4_phoId', 'L1tkElectron_4_saId', 'L1tkElectron_4_hwQual', 'L1tkElectron_4_charge', 'L1tkElectron_4_eta', 'L1tkElectron_4_phi', 'L1tkElectron_4_pt', 'L1tkElectron_4_relIso', 'L1tkElectron_4_z0', 'L1tkPhoton_0_eleId', 'L1tkPhoton_0_phoId', 'L1tkPhoton_0_saId', 'L1tkPhoton_0_hwQual', 'L1tkPhoton_0_eta', 'L1tkPhoton_0_phi', 'L1tkPhoton_0_pt', 'L1tkPhoton_0_relIso', 'L1tkPhoton_1_eleId', 'L1tkPhoton_1_phoId', 'L1tkPhoton_1_saId', 'L1tkPhoton_1_hwQual', 'L1tkPhoton_1_eta', 'L1tkPhoton_1_phi', 'L1tkPhoton_1_pt', 'L1tkPhoton_1_relIso', 'L1tkPhoton_2_eleId', 'L1tkPhoton_2_phoId', 'L1tkPhoton_2_saId', 'L1tkPhoton_2_hwQual', 'L1tkPhoton_2_eta', 'L1tkPhoton_2_phi', 'L1tkPhoton_2_pt', 'L1tkPhoton_2_relIso', 'L1tkPhoton_3_eleId', 'L1tkPhoton_3_phoId', 'L1tkPhoton_3_saId', 'L1tkPhoton_3_hwQual', 'L1tkPhoton_3_eta', 'L1tkPhoton_3_phi', 'L1tkPhoton_3_pt', 'L1tkPhoton_3_relIso', 'L1tkPhoton_4_eleId', 'L1tkPhoton_4_phoId', 'L1tkPhoton_4_saId', 'L1tkPhoton_4_hwQual', 'L1tkPhoton_4_eta', 'L1tkPhoton_4_phi', 'L1tkPhoton_4_pt', 'L1tkPhoton_4_relIso', 'L1puppiJetHisto_0_eta', 'L1puppiJetHisto_0_phi', 'L1puppiJetHisto_0_pt', 'L1puppiJetHisto_1_eta', 'L1puppiJetHisto_1_phi', 'L1puppiJetHisto_1_pt', 'L1puppiJetHisto_2_eta', 'L1puppiJetHisto_2_phi', 'L1puppiJetHisto_2_pt', 'L1puppiJetHisto_3_eta', 'L1puppiJetHisto_3_phi', 'L1puppiJetHisto_3_pt', 'L1puppiJetHisto_4_eta', 'L1puppiJetHisto_4_phi', 'L1puppiJetHisto_4_pt', 'L1gmtTkMuon_0_charge', 'L1gmtTkMuon_0_hwBeta', 'L1gmtTkMuon_0_hwEta', 'L1gmtTkMuon_0_hwPhi', 'L1gmtTkMuon_0_hwPt', 'L1gmtTkMuon_0_hwQual', 'L1gmtTkMuon_0_z0', 'L1gmtTkMuon_1_charge', 'L1gmtTkMuon_1_hwBeta', 'L1gmtTkMuon_1_hwEta', 'L1gmtTkMuon_1_hwPhi', 'L1gmtTkMuon_1_hwPt', 'L1gmtTkMuon_1_hwQual', 'L1gmtTkMuon_1_z0', 'L1gmtTkMuon_2_charge', 'L1gmtTkMuon_2_hwBeta', 'L1gmtTkMuon_2_hwEta', 'L1gmtTkMuon_2_hwPhi', 'L1gmtTkMuon_2_hwPt', 'L1gmtTkMuon_2_hwQual', 'L1gmtTkMuon_2_z0', 'L1caloJet_0_eta', 'L1caloJet_0_phi', 'L1caloJet_0_pt', 'L1caloJet_1_eta', 'L1caloJet_1_phi', 'L1caloJet_1_pt', 'L1caloJet_2_eta', 'L1caloJet_2_phi', 'L1caloJet_2_pt', 'L1caloJet_3_eta', 'L1caloJet_3_phi', 'L1caloJet_3_pt']\n"
     ]
    }
   ],
   "source": [
    "truejetsnames = [\"calojets_\" + i for i in truejets.fields]\n",
    "truetausnames = [\"calotaus_\" + i for i in truetaus.fields]\n",
    "\n",
    "names =  truetausnames  + drfnames\n",
    "print(len(names))\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare trues:\n",
    "#broadcast other objects to the form of truetaus:\n",
    "#first flip axis\n",
    "objswapped =np.swapaxes(objarray,0,1)\n",
    "objswappedak = ak.Array(objswapped)\n",
    "#output arrays\n",
    "tobj,b = ak.broadcast_arrays(objswappedak[:,np.newaxis],falsetaus)\n",
    "tobjflat =ak.flatten(tobj)\n",
    "nptobjflat = ak.to_numpy(tobjflat)\n",
    "#transform truetaus to numpy\n",
    "ttflat = ak.flatten(falsetaus)\n",
    "npttflat = ak.to_numpy(ttflat)\n",
    "tjflat = ak.flatten(falsejets)\n",
    "nptjflat = ak.to_numpy(tjflat)\n",
    "\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "unstructuredtruetaus = structured_to_unstructured(npttflat)\n",
    "unstructuredtruejets = structured_to_unstructured(nptjflat)\n",
    "\n",
    "falses = np.concatenate((unstructuredtruetaus,drfvals),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.92578125\n",
      "-3.9257812\n",
      "1.43994140625\n",
      "1.4399414\n",
      "-0.392822265625\n",
      "-0.39282227\n",
      "1.43994140625\n",
      "[1.09, -2.57, -1.53, 1.44, -2.84, -0.829]\n",
      "[529, 324, 102]\n",
      "-639.96533203125\n"
     ]
    }
   ],
   "source": [
    "indexlist = drfnames\n",
    "print(falses[0][0])\n",
    "print(standardizeddict[\"L1caloJet\"].eta[0][0])\n",
    "print(falses[0][1])\n",
    "print(standardizeddict[\"L1caloJet\"].phi[0][0])\n",
    "\n",
    "\n",
    "print(trues[0][0])\n",
    "print(standardizeddict[\"L1caloJet\"].eta[1][1])\n",
    "print(falses[0][1])\n",
    "print(standardizeddict[\"L1caloJet\"].phi[1])\n",
    "idx = indexlist.index(\"L1gmtMuon_0_hwPt\")\n",
    "\n",
    "print(standardizeddict[\"L1gmtTkMuon\"].hwPt[0])\n",
    "tt = falses[0]\n",
    "\n",
    "print(np.sum(trues[:,0]))\n",
    "for i in range(len(trues[0])):\n",
    "    if np.max(trues[:,i]) == np.min(trues[:,i]):\n",
    "        print(names[i])\n",
    "# tt.where(529)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # loop through all events\n",
    "# # for ievt in tqdm(range(100)):\n",
    "# #     cut_deltaRl1jets_nnTaus = l1jets_near_nnTaus[ievt]\n",
    "    \n",
    "# #     # loop through each l1jet near an nnTau in the event\n",
    "# #     for ijet in np.where(ak.any(cut_deltaRl1jets_nnTaus,axis=1))[0]:\n",
    "# #         # for each l1jet loop through the taus near it and save the pair with their dR and the Tau truth value\n",
    "# #         for itau in np.where(cut_deltaRl1jets_nnTaus[ijet])[0]:\n",
    "\n",
    "# #             input_jets.append( [l1jets[ievt][ijet]['pt'], l1jets[ievt][ijet]['eta'], l1jets[ievt][ijet]['phi']])\n",
    "# #             input_taus.append( [\n",
    "# #                                 nntaus[ievt][itau]['pt'], nntaus[ievt][itau]['eta'], nntaus[ievt][itau]['phi'],\n",
    "# #                                 nntaus[ievt][itau]['fulliso'], nntaus[ievt][itau]['z0'], nntaus[ievt][itau]['passtightnn'],\n",
    "# #                                 nntaus[ievt][itau]['chg'], nntaus[ievt][itau]['dxy'], nntaus[ievt][itau]['passloosenn']\n",
    "# #                                ])\n",
    "# #             input_dRs.append(deltaRl1jets_nnTaus[ievt][ijet][itau])\n",
    "# #             input_ptperc.append(ptpercentage[ievt][ijet][itau])\n",
    "            \n",
    "# #             if len(nntaus_near_taus[ievt]):\n",
    "# #                 input_is_true_tau.append(nntaus_near_taus[ievt][itau])\n",
    "# #                 input_muons.append(muons[ievt])\n",
    "# #                 input_eles_barrel.append(elecsbarrel_gt[ievt])\n",
    "# #                 input_eles_endcap.append(elecsendcap_gt[ievt])\n",
    "\n",
    "# #             else:\n",
    "# #                 input_is_true_tau.append(False)\n",
    "# #                 input_muons.append(muons[ievt])\n",
    "# #                 input_eles_barrel.append(elecsbarrel_gt[ievt])\n",
    "# #                 input_eles_endcap.append(elecsendcap_gt[ievt])\n",
    "# true_objs = []\n",
    "# true_taus = []\n",
    "# true_jets = []\n",
    "# false_taus = []\n",
    "# false_objs = []\n",
    "# false_jets = []\n",
    "# dRtruejettaus = []\n",
    "# dRfalsejettaus = []\n",
    "# debug = 0\n",
    "# varnamestaus = nntaus.fields\n",
    "# varnamesjets = l1jets.fields\n",
    "# #loop through all events:\n",
    "# for ievt in tqdm(range(len(nntaus))):\n",
    "# # for ievt in tqdm(range(5000)):\n",
    "\n",
    "#         #loop through all taus:\n",
    "#         for itau in range(len(nntaus[ievt])):\n",
    "        \n",
    "#             #check if there is a tau in the event:\n",
    "#             if len(nntaus_near_taus[ievt]):\n",
    "#                 # append only taus with a delta r smaller than 0.3:\n",
    "#                 if nntaus_near_taus[ievt][itau]:\n",
    "#                     true_taus.append( \n",
    "#                                     list(nntaus[ievt][itau].to_list().values())\n",
    "#                                     )    \n",
    "#                     true_objs.append(objarray[:,ievt]) \n",
    "#                     closestjet = np.argmin(deltaRnnTaus_l1jets[ievt][itau])\n",
    "#                     dRtruejettaus.append(closestjet)\n",
    "#                     true_jets.append( [l1jets[ievt][closestjet]['pt'], l1jets[ievt][closestjet]['eta'], l1jets[ievt][closestjet]['phi']])\n",
    "#                 else:\n",
    "#                 # tau delta r bigger:\n",
    "#                     false_taus.append(\n",
    "#                                     list(nntaus[ievt][itau].to_list().values())\n",
    "#                                     )    \n",
    "#                     false_objs.append(objarray[:,ievt]) \n",
    "#                     closestjet = np.argmin(deltaRnnTaus_l1jets[ievt][itau])\n",
    "#                     dRfalsejettaus.append(closestjet)\n",
    "\n",
    "#                     false_jets.append( [l1jets[ievt][closestjet]['pt'], l1jets[ievt][closestjet]['eta'], l1jets[ievt][closestjet]['phi']])\n",
    "#             else:\n",
    "#                 # no jet:\n",
    "\n",
    "#                 false_taus.append(\n",
    "#                                 list(nntaus[ievt][itau].to_list().values())\n",
    "#                                 )    \n",
    "#                 false_objs.append(objarray[:,ievt]) \n",
    "#                 closestjet = np.argmin(deltaRnnTaus_l1jets[ievt][itau])\n",
    "#                 dRfalsejettaus.append(closestjet)\n",
    "#                 if type(l1jets[ievt][closestjet]) is float:\n",
    "#                     print('strange behaviour',ievt)\n",
    "#                     false_jets.append([0,0,0])\n",
    "#                 else:\n",
    "#                     false_jets.append( [l1jets[ievt][closestjet]['pt'], l1jets[ievt][closestjet]['eta'], l1jets[ievt][closestjet]['phi']])\n",
    "\n",
    "#                 if debug:\n",
    "#                     print(\"NN tau:\",nntaus[ievt])\n",
    "#                     print(\"Gen Tau\", deltaRtaus_nnTaus[ievt])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug =  1\n",
    "# if debug:\n",
    "#     print(v4l1jets[0].phi)\n",
    "#     print(deltaRl1jets_nnTaus[0])\n",
    "#     print(nntaus[0].L1nnPuppiTau_pt)\n",
    "#     print(deltaRnnTaus_l1jets[0][0])\n",
    "#     print(deltaRl1jets_nnTaus[0])\n",
    "#     print(ptpercentage[0])\n",
    "#     np.argmin(deltaRnnTaus_l1jets[0][0])\n",
    "#     # show me the jet that has the smallest delta R to a tau:\n",
    "#     # for this we select the first tau and print the jet that has the smallest delta r\n",
    "\n",
    "#     print(nntaus_near_taus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(false_jets[0])\n",
    "# print(false_jets[65281])\n",
    "# false_jets[65281]\n",
    "# print(ak.is_none(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{eta: -2.39, phi: -0.829, pt: 69.8}, {...}, {eta: 2.13, phi: -0.131, ...}]\n"
     ]
    }
   ],
   "source": [
    "print(l1jets[3404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #convert to array:\n",
    "\n",
    "\n",
    "# #temp workaround: i think there was no jet:\n",
    "# # false_jets[3403] = [0.0,0.0,0.0]\n",
    "# false_jets[64710] = [0.0,0.0,0.0]\n",
    "\n",
    "\n",
    "# false_taus_arr = np.vstack(false_taus)\n",
    "# false_jets_arr = np.vstack(false_jets)\n",
    "# false_objs_arr = np.vstack(false_objs)\n",
    "# false_drjettau_arr = np.array(dRfalsejettaus)\n",
    "# false_drjettau_arr = false_drjettau_arr.reshape(len(false_drjettau_arr),1)\n",
    "\n",
    "# true_taus_arr = np.vstack(true_taus)\n",
    "# true_jets_arr = np.vstack(true_jets)\n",
    "# true_objs_arr = np.vstack(true_objs)\n",
    "# true_drjettau_arr = np.array(dRtruejettaus)\n",
    "# true_drjettau_arr = true_drjettau_arr.reshape(len(true_drjettau_arr),1)\n",
    "\n",
    "# if debug:\n",
    "#     print(\"false objects dimensions:\")\n",
    "#     print(\"taus: \",false_taus_arr.shape)\n",
    "#     print(\"jets: \",false_jets_arr.shape)\n",
    "#     print(\"objs: \",false_objs_arr.shape)\n",
    "#     print(\"drjt: \",false_drjettau_arr.shape)\n",
    "#     print(\"true objects dimensions:\")\n",
    "#     print(\"taus: \",true_taus_arr.shape)\n",
    "#     print(\"jets: \",true_jets_arr.shape)\n",
    "#     print(\"objs: \",true_objs_arr.shape)\n",
    "#     print(\"drjt: \",true_drjettau_arr.shape)\n",
    "\n",
    "\n",
    "\n",
    "# x_bkg = np.concatenate((false_taus_arr,false_jets_arr,false_objs_arr,false_drjettau_arr),axis=1)\n",
    "# x_sig = np.concatenate((true_taus_arr,true_jets_arr,true_objs_arr,true_drjettau_arr),axis=1)\n",
    "# featurenames = varnamestaus + varnamesjets + inpnames + [\"dr_jet_tau\"]\n",
    "\n",
    "# featurenames\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig = trues\n",
    "x_bkg = falses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.92578125\n",
      "-3.9257812\n",
      "1.43994140625\n",
      "1.4399414\n",
      "['L1gmtMuon_0_charge', 'L1gmtMuon_0_hwBeta', 'L1gmtMuon_0_hwEta', 'L1gmtMuon_0_hwPhi', 'L1gmtMuon_0_hwPt', 'L1gmtMuon_0_hwQual', 'L1gmtMuon_1_charge', 'L1gmtMuon_1_hwBeta', 'L1gmtMuon_1_hwEta', 'L1gmtMuon_1_hwPhi', 'L1gmtMuon_1_hwPt', 'L1gmtMuon_1_hwQual', 'L1tkElectron_0_eleId', 'L1tkElectron_0_phoId', 'L1tkElectron_0_saId', 'L1tkElectron_0_hwQual', 'L1tkElectron_0_charge', 'L1tkElectron_0_eta', 'L1tkElectron_0_phi', 'L1tkElectron_0_pt', 'L1tkElectron_0_relIso', 'L1tkElectron_0_z0', 'L1tkElectron_1_eleId', 'L1tkElectron_1_phoId', 'L1tkElectron_1_saId', 'L1tkElectron_1_hwQual', 'L1tkElectron_1_charge', 'L1tkElectron_1_eta', 'L1tkElectron_1_phi', 'L1tkElectron_1_pt', 'L1tkElectron_1_relIso', 'L1tkElectron_1_z0', 'L1tkElectron_2_eleId', 'L1tkElectron_2_phoId', 'L1tkElectron_2_saId', 'L1tkElectron_2_hwQual', 'L1tkElectron_2_charge', 'L1tkElectron_2_eta', 'L1tkElectron_2_phi', 'L1tkElectron_2_pt', 'L1tkElectron_2_relIso', 'L1tkElectron_2_z0', 'L1tkElectron_3_eleId', 'L1tkElectron_3_phoId', 'L1tkElectron_3_saId', 'L1tkElectron_3_hwQual', 'L1tkElectron_3_charge', 'L1tkElectron_3_eta', 'L1tkElectron_3_phi', 'L1tkElectron_3_pt', 'L1tkElectron_3_relIso', 'L1tkElectron_3_z0', 'L1tkElectron_4_eleId', 'L1tkElectron_4_phoId', 'L1tkElectron_4_saId', 'L1tkElectron_4_hwQual', 'L1tkElectron_4_charge', 'L1tkElectron_4_eta', 'L1tkElectron_4_phi', 'L1tkElectron_4_pt', 'L1tkElectron_4_relIso', 'L1tkElectron_4_z0', 'L1tkPhoton_0_eleId', 'L1tkPhoton_0_phoId', 'L1tkPhoton_0_saId', 'L1tkPhoton_0_hwQual', 'L1tkPhoton_0_eta', 'L1tkPhoton_0_phi', 'L1tkPhoton_0_pt', 'L1tkPhoton_0_relIso', 'L1tkPhoton_1_eleId', 'L1tkPhoton_1_phoId', 'L1tkPhoton_1_saId', 'L1tkPhoton_1_hwQual', 'L1tkPhoton_1_eta', 'L1tkPhoton_1_phi', 'L1tkPhoton_1_pt', 'L1tkPhoton_1_relIso', 'L1tkPhoton_2_eleId', 'L1tkPhoton_2_phoId', 'L1tkPhoton_2_saId', 'L1tkPhoton_2_hwQual', 'L1tkPhoton_2_eta', 'L1tkPhoton_2_phi', 'L1tkPhoton_2_pt', 'L1tkPhoton_2_relIso', 'L1tkPhoton_3_eleId', 'L1tkPhoton_3_phoId', 'L1tkPhoton_3_saId', 'L1tkPhoton_3_hwQual', 'L1tkPhoton_3_eta', 'L1tkPhoton_3_phi', 'L1tkPhoton_3_pt', 'L1tkPhoton_3_relIso', 'L1tkPhoton_4_eleId', 'L1tkPhoton_4_phoId', 'L1tkPhoton_4_saId', 'L1tkPhoton_4_hwQual', 'L1tkPhoton_4_eta', 'L1tkPhoton_4_phi', 'L1tkPhoton_4_pt', 'L1tkPhoton_4_relIso', 'L1puppiJetHisto_0_eta', 'L1puppiJetHisto_0_phi', 'L1puppiJetHisto_0_pt', 'L1puppiJetHisto_1_eta', 'L1puppiJetHisto_1_phi', 'L1puppiJetHisto_1_pt', 'L1puppiJetHisto_2_eta', 'L1puppiJetHisto_2_phi', 'L1puppiJetHisto_2_pt', 'L1puppiJetHisto_3_eta', 'L1puppiJetHisto_3_phi', 'L1puppiJetHisto_3_pt', 'L1puppiJetHisto_4_eta', 'L1puppiJetHisto_4_phi', 'L1puppiJetHisto_4_pt', 'L1gmtTkMuon_0_charge', 'L1gmtTkMuon_0_hwBeta', 'L1gmtTkMuon_0_hwEta', 'L1gmtTkMuon_0_hwPhi', 'L1gmtTkMuon_0_hwPt', 'L1gmtTkMuon_0_hwQual', 'L1gmtTkMuon_0_z0', 'L1gmtTkMuon_1_charge', 'L1gmtTkMuon_1_hwBeta', 'L1gmtTkMuon_1_hwEta', 'L1gmtTkMuon_1_hwPhi', 'L1gmtTkMuon_1_hwPt', 'L1gmtTkMuon_1_hwQual', 'L1gmtTkMuon_1_z0', 'L1gmtTkMuon_2_charge', 'L1gmtTkMuon_2_hwBeta', 'L1gmtTkMuon_2_hwEta', 'L1gmtTkMuon_2_hwPhi', 'L1gmtTkMuon_2_hwPt', 'L1gmtTkMuon_2_hwQual', 'L1gmtTkMuon_2_z0', 'L1caloJet_0_eta', 'L1caloJet_0_phi', 'L1caloJet_0_pt', 'L1caloJet_1_eta', 'L1caloJet_1_phi', 'L1caloJet_1_pt', 'L1caloJet_2_eta', 'L1caloJet_2_phi', 'L1caloJet_2_pt', 'L1caloJet_3_eta', 'L1caloJet_3_phi', 'L1caloJet_3_pt']\n",
      "-0.392822265625\n",
      "-0.39282227\n",
      "1.43994140625\n",
      "[1.09, -2.57, -1.53, 1.44, -2.84, -0.829]\n",
      "[529, 324, 102]\n",
      "['charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'eleId', 'phoId', 'saId', 'hwQual', 'charge', 'eta', 'phi', 'pt', 'relIso', 'z0', 'eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'relIso', 'eta', 'phi', 'pt', 'eleId', 'phoId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'eleId', 'saId', 'hwQual', 'eta', 'phi', 'pt', 'charge', 'hwBeta', 'hwEta', 'hwPhi', 'hwPt', 'hwQual', 'z0']\n",
      "121\n",
      "128\n",
      "150\n",
      "(array([121]),)\n",
      "(array([128]),)\n",
      "(array([135]),)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indexlist = drfnames\n",
    "print(falses[0][0])\n",
    "print(standardizeddict[\"L1caloJet\"].eta[0][0])\n",
    "print(falses[0][1])\n",
    "print(standardizeddict[\"L1caloJet\"].phi[0][0])\n",
    "\n",
    "print(drfnames)\n",
    "print(trues[0][0])\n",
    "print(standardizeddict[\"L1caloJet\"].eta[1][1])\n",
    "print(falses[0][1])\n",
    "print(standardizeddict[\"L1caloJet\"].phi[1])\n",
    "idx = indexlist.index(\"L1gmtTkMuon_0_hwPt\")\n",
    "\n",
    "print(standardizeddict[\"L1gmtTkMuon\"].hwPt[0])\n",
    "\n",
    "tt = falses[0]\n",
    "print(ls)\n",
    "# print(drfvals[0])\n",
    "print(idx)\n",
    "idx = indexlist.index(\"L1gmtTkMuon_1_hwPt\")\n",
    "print(idx)\n",
    "print(len(drfvals[0]))\n",
    "print(np.where(drfvals[0] == 529))\n",
    "print(np.where(drfvals[0] == 324))\n",
    "print(np.where(drfvals[0] == 102))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88096\n",
      "251829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(153,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(trues))\n",
    "print(len(falses))\n",
    "trues[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # creating labels\n",
    "y_bkg = np.zeros(len(x_bkg))\n",
    "y_sig = np.ones(len(x_sig))\n",
    "\n",
    "# combining signal & bkg\n",
    "x = np.concatenate((x_bkg, x_sig))\n",
    "y = np.concatenate((y_bkg, y_sig))\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# _ = scaler.fit(x)\n",
    "# x_scaled = scaler.transform(x)\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# pca = PCA(n_components=2)  # You can choose the number of components\n",
    "# X_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# # Step 5: Analyze the results\n",
    "# # Explained variance\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "# print('Explained variance by each component:', explained_variance)\n",
    "\n",
    "# # Plotting the principal components, colored by target\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.title('PCA of Dataset')\n",
    "# plt.colorbar(scatter)\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "_ = scaler.fit(x)\n",
    "x_scaled = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:   smallnet_outputs_for_training_2024_08_24-10_36_14_AM/\n"
     ]
    }
   ],
   "source": [
    " # make directory\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "datenow = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "outdir = \"smallnet_outputs_for_training_{}/\".format(datenow)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "with open(outdir+\"scaler.pkl\", 'wb') as file_pi: \n",
    "   pickle.dump(scaler, file_pi)\n",
    "\n",
    "print(\"outdir:  \",outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(outdir+\"featurenames.pkl\", 'wb') as file_pi: \n",
    "   pickle.dump(names, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(names)\n",
    "# with open(outdir+\"names.pkl\", 'wb') as file_pi: \n",
    "#    pickle.dump(names, file_pi)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ak.to_parquet(X_train, outdir+\"X_train_scaled.parquet\")\n",
    "# ak.to_parquet(y_train, outdir+\"y_train_scaled.parquet\")\n",
    "# ak.to_parquet(X_test, outdir+\"X_test_scaled.parquet\")\n",
    "# ak.to_parquet(y_test, outdir+\"y_test_scaled.parquet\")# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "{0: 0.6760281873143827, 1: 1.9202270887077033}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.4)\n",
    "print(X_train.shape[1])\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#weighing trues and false\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Convert class_weights to a dictionary\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f62c77e9a90>\n",
       "  created_by: parquet-cpp-arrow version 16.0.0\n",
       "  num_columns: 1\n",
       "  num_rows: 135970\n",
       "  num_row_groups: 1\n",
       "  format_version: 2.6\n",
       "  serialized_size: 0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.to_parquet(X_train, outdir+\"X_train_scaled.parquet\")\n",
    "ak.to_parquet(y_train, outdir+\"y_train_scaled.parquet\")\n",
    "ak.to_parquet(X_test, outdir+\"X_test_scaled.parquet\")\n",
    "ak.to_parquet(y_test, outdir+\"y_test_scaled.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truetaus.fields\n",
    "truetaus_plot =ak.flatten(truetaus,axis=-1)\n",
    "truetaus_plot.type.show()\n",
    "\n",
    "falsetaus_plot =ak.flatten(falsetaus,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for i in truetaus.fields:\n",
    "    # if \"_0_\" not in name: continue\n",
    "        \n",
    "    plt.figure()\n",
    "    \n",
    "    _ = plt.hist(falsetaus_plot[i], bins = 100, log = True, density = False, label = \"Bkg\")\n",
    "    _ = plt.hist(truetaus_plot[i], bins = _[1], histtype = \"step\", density = False, label = \"Sig\")\n",
    "    \n",
    "    plt.xlabel(i)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preparemindrplot(a,b,c,d,sort):\n",
    "    f,g = mapObjects(a,b)\n",
    "    drmu = DeltaRcalc(c,d)\n",
    "    tts = ak.min(drmu,axis=1)\n",
    "    return ak.flatten(tts,axis=None)\n",
    "    # muonssorted = g[tts]\n",
    "    # test = ak.any(nntaus_near_taus,axis=1)\n",
    "    # mm = ak.flatten(muonssorted[ak.any(sort,axis=1)],axis=1)\n",
    "\n",
    "y = [\n",
    " \"L1gmtMuon\",\n",
    "\"L1gmtTkMuon\",\n",
    "\"L1tkPhoton\",\n",
    "\"L1tkElectron\",\n",
    "\"L1puppiJetHisto\",\n",
    "\"L1EGbarrel\",\n",
    "\"L1EGendcap\"]\n",
    "for x in y:\n",
    "    # standardizeddict[x] = ak.without_field(standardizeddict[x],['eta'])\n",
    "    print(standardizeddict[x].fields)\n",
    "    truemuons_plot =  Preparemindrplot(truetaus,standardizeddict[x],v4truetaus,fourvectordict[x],nntaus_near_taus)\n",
    "    falsemuons_plot = Preparemindrplot(falsetaus,standardizeddict[x],v4falsetaus,fourvectordict[x],~nntaus_near_taus)\n",
    "    # standardizeddict[\"L1gmtTkMuon\"]\n",
    "    print(ak.flatten(truemuons_plot,axis=None))\n",
    "\n",
    "    plt.figure()\n",
    "        \n",
    "    _ = plt.hist(ak.flatten(truemuons_plot,axis=None), bins = 50, log = False, density = True, label = \"Sig\")\n",
    "    _ = plt.hist(ak.flatten(falsemuons_plot,axis=None), bins = _[1], histtype = \"step\", density = True, label = \"Bkg\")\n",
    "        \n",
    "    plt.xlabel(\"dR\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(x)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu1'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu2'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu3'))\n",
    "model.add(Dense(1, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='sigmoid', name='sigmoid'))\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=adam, loss=['binary_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(\n",
    "model, to_file='{}model.png'.format(outdir), show_shapes=True, show_dtype=True,\n",
    "show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    class_weight=class_weight_dict,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    # callbacks=callbacks.callbacks,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "def plotTrainingHistory(history, metrics = [\"loss\", \"accuracy\"], f = None, axs = None):\n",
    "    \n",
    "    # creating the plot\n",
    "    if not f and not axs: \n",
    "        f, axs = plt.subplots(len(metrics), 1, figsize = (12, 4*len(metrics)), sharex = True)\n",
    "    if len(metrics) == 1:\n",
    "        axs = [axs]\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # labeling\n",
    "#     hep.cms.label(\"private work\", data=False, ax=axs[0])\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "        \n",
    "        metric = metrics[i]\n",
    "        ax = axs[i]\n",
    "        ax.set_ylabel(metric)\n",
    "        \n",
    "        if isinstance(history, list): # handle kfold\n",
    "            for foldi in range(len(history)):\n",
    "                ax.plot(history[foldi].history[metric], color = \"C{}\".format(foldi))\n",
    "                ax.plot(history[foldi].history['val_' + metric], color = \"C{}\".format(foldi), linestyle = \"--\")\n",
    "                \n",
    "            la2, = ax.plot([0,0], [0,0], color=\"Grey\")\n",
    "            lb2, = ax.plot([0,0], [0,0], color=\"Grey\", linestyle = \"--\")\n",
    "            ax.legend([la2, lb2], [\"training\", \"validation\"])\n",
    "        else: \n",
    "            xs = np.arange(len(history.history['val_' + metric]))\n",
    "            ax.plot(xs,history.history[metric], label = 'training')\n",
    "            ax.plot(xs+.5, history.history['val_' + metric], label= 'validation')\n",
    "            ax.legend()\n",
    "\n",
    "    axs[-1].set_xlabel(\"Epoch\")\n",
    "    \n",
    "    return f, axs\n",
    "\n",
    "\n",
    "model.save(outdir + \"/model.h5\")\n",
    "\n",
    "with open(outdir + \"/history.pkl\", 'wb') as file_pi: pickle.dump(history, file_pi)\n",
    "\n",
    "fig = plotTrainingHistory(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)  # Save predictions:\n",
    "ak.to_parquet(y_test_pred, outdir+\"y_test_pred.parquet\")  \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_test_pred, drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## for L1 rate estimates from ZeroBias/SingleNuMC\n",
    "def totalMinBiasRate(nCollBunch = 2500):\n",
    "    LHCfreq = 11245.6\n",
    "    return LHCfreq * nCollBunch / 1e3 # in kHz\n",
    "\n",
    "\n",
    "plt.plot(fpr * totalMinBiasRate(), tpr)\n",
    "plt.xlabel(\"FPR*MBrate: Trigger rate [kHz]\")\n",
    "plt.ylabel(\"TPR: signal efficiency\")\n",
    "plt.grid()\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "y_test_pred = model.predict(X_test)  # Save predictions:\n",
    "ak.to_parquet(y_test_pred, outdir+\"y_test_pred.parquet\")  \n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_test_pred, drop_intermediate=False)  \n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR: background efficiency\")\n",
    "plt.ylabel(\"TPR: signal efficiency\")\n",
    "plt.grid()\n",
    "plt.savefig(outdir+'efficiency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for L1 rate estimates from ZeroBias/SingleNuMC\n",
    "def totalMinBiasRate(nCollBunch = 2500):\n",
    "    LHCfreq = 11245.6\n",
    "    return LHCfreq * nCollBunch / 1e3 # in kHz  Let's now make the ROC plot and focus on a rate range 0-100 kHz: plt.plot(fpr * totalMinBiasRate(), tpr)\n",
    "plt.plot(fpr * totalMinBiasRate(), tpr)\n",
    "plt.xlabel(\"FPR*MBrate: Trigger rate [kHz]\")\n",
    "plt.ylabel(\"TPR: signal efficiency\")\n",
    "plt.grid()\n",
    "plt.xlim(0,1000)\n",
    "plt.savefig(outdir+'atrate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Dense Layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units_fc1', min_value=128, max_value=512, step=64),\n",
    "        input_shape=(X_train.shape[1],),\n",
    "        name='fc1',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(hp.Float('reg1', 0.0001, 0.001, step=0.0001))\n",
    "    ))\n",
    "    model.add(Activation(activation='relu', name='relu1'))\n",
    "    model.add(Dropout(hp.Float('dropout_fc1', 0.2, 0.5, step=0.1)))\n",
    "    \n",
    "    # Second Dense Layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units_fc2', min_value=32, max_value=128, step=32),\n",
    "        name='fc2',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(hp.Float('reg2', 0.0001, 0.001, step=0.0001))\n",
    "    ))\n",
    "    model.add(Activation(activation='relu', name='relu2'))\n",
    "    model.add(Dropout(hp.Float('dropout_fc2', 0.2, 0.5, step=0.1)))\n",
    "    \n",
    "    # Third Dense Layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units_fc3', min_value=16, max_value=128, step=16),\n",
    "        name='fc3',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(hp.Float('reg3', 0.0001, 0.0004, step=0.0001))\n",
    "    ))\n",
    "    model.add(Activation(activation='relu', name='relu3'))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(1, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "    model.add(Activation(activation='sigmoid', name='sigmoid'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', \n",
    "                                 values=[1e-2, 1e-3, 1e-4]\n",
    "                                 )\n",
    "    optimizers_dict = {\n",
    "        \"Adam\":    Adam(learning_rate=hp_learning_rate),\n",
    "        \"SGD\":     SGD(learning_rate=hp_learning_rate),\n",
    "        \"Adagrad\": Adagrad(learning_rate=hp_learning_rate)\n",
    "        }\n",
    "\n",
    "    hp_optimizers = hp.Choice(\n",
    "        'optimizer', \n",
    "        values=[\"Adam\"]\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers_dict[hp_optimizers],\n",
    "        loss=['binary_crossentropy'], metrics=['accuracy']\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=2,\n",
    "    directory='opt9',\n",
    "    project_name='withmorevars',\n",
    "\n",
    ")\n",
    "\n",
    "# Load your dataset here\n",
    "# X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "# Define the search space for algorithm parameters\n",
    "def hyperparameters(hp):\n",
    "    return {\n",
    "        'batch_size': hp.Int('batch_size', min_value=16, max_value=128, step=16),\n",
    "        'learning_rate': hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log'),\n",
    "    }\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_test, y_test), \n",
    "                 class_weight=class_weight_dict,\n",
    "    validation_split=0.2,\n",
    "             callbacks=[\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "])\n",
    "\n",
    "# Retrieve and print the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# print(f\"\"\"\n",
    "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "# layer is {best_hps.get('units_fc1')}, the second layer is {best_hps.get('units_fc2')}, \n",
    "# the third layer is {best_hps.get('units_fc3')}, \n",
    "# the optimal dropout rate for the first layer is {best_hps.get('dropout_fc1')},\n",
    "# the second layer is {best_hps.get('dropout_fc2')},\n",
    "# the best optimizer is {best_hps.get('optimizer')},\n",
    "# the best batch size is {best_hps.get('batch_size')},\n",
    "# and the best learning rate is {best_hps.get('learning_rate')}.\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units_fc1')}, the second layer is {best_hps.get('units_fc2')}, \n",
    "the third layer is {best_hps.get('units_fc3')}, \n",
    "the optimal dropout rate for the first layer is {best_hps.get('dropout_fc1')},\n",
    "the second layer is {best_hps.get('dropout_fc2')},\n",
    "the best optimizer is {best_hps.get('optimizer')},\n",
    "and the best learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the tuner\n",
    "# tuner = RandomSearch(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_trials=10,\n",
    "#     executions_per_trial=1,\n",
    "#     directory='test9',\n",
    "#     project_name='tuning_demo'\n",
    "# )\n",
    "\n",
    "# # Load your dataset here\n",
    "# # X_train, y_train, X_val, y_val = ...\n",
    "\n",
    "# # Start the hyperparameter search\n",
    "# tuner.search(X_train, y_train, epochs=40, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_hps())\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"units_fc1: {best_hps.get('units_fc1')}\")\n",
    "print(f\"units_fc2: {best_hps.get('units_fc2')}\")\n",
    "print(f\"units_fc3: {best_hps.get('units_fc3')}\")\n",
    "print(f\"dropout_fc1: {best_hps.get('dropout_fc1')}\")\n",
    "print(f\"dropout_fc2: {best_hps.get('dropout_fc2')}\")\n",
    "print(f\"optimizer: {best_hps.get('optimizer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "# history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=32)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(outdir, histogram_freq=1)\n",
    "print(outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.save(filepath=outdir+\"model\")\n",
    "%time\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=1000,\n",
    "    class_weight=class_weight_dict,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    callbacks=tensorboard_callback,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=featurenames, class_names=[0, 1], mode='classification')\n",
    "exp = explainer.explain_instance(X_test[5], model.predict, num_features=20, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.available_labels()\n",
    "explainer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# def predict_function(x):\n",
    "#     # Assuming your Keras model takes input x and returns predictions\n",
    "#     return model.predict(x)\n",
    "\n",
    "# # Step 5: Generate SHAP explanations\n",
    "# # Create a DeepExplainer object by passing the Keras model and the training data\n",
    "# explainer = shap.DeepExplainer(model, X_train)\n",
    "\n",
    "# # Explain predictions on the first instance from the test set\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# # Step 6: Visualize feature importance\n",
    "# shap.initjs()  # Initialize JavaScript for visualization (for Jupyter Notebook)\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0], X_test[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
